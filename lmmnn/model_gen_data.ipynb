{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import generate_data\n",
    "from layers import NLL\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense,LSTM,GRU,Dropout, Flatten, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nn import reg_nn_lmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"n_fixed_effects\": 10,\n",
    "    \"fixed_intercept\": 1,\n",
    "    \"X_non_linear\": True,\n",
    "    \"Z_non_linear\": False,\n",
    "    \"Z_embed_dim_pct\": 10,\n",
    "    \"n_per_cat\": 3,\n",
    "}\n",
    "\n",
    "# qs=[100,1000,10000] # use this if you want 3 z variables\n",
    "# sig2bs=[0.1,1,10]\n",
    "X_train, X_test, y_train, y_test, x_cols, dist_matrix, time2measure_dict = generate_data(mode='glmm', qs=[100], sig2e=1, sig2bs=[0.1], sig2bs_spatial=[], q_spatial=[], N=100000, rhos=None, p_censor=None, params=params)#\"./conf_files/conf_glmm.conf_glmm.yaml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>z0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81168</th>\n",
       "      <td>-0.531767</td>\n",
       "      <td>-0.531129</td>\n",
       "      <td>-0.318579</td>\n",
       "      <td>-0.524463</td>\n",
       "      <td>-0.704684</td>\n",
       "      <td>-0.711323</td>\n",
       "      <td>0.280344</td>\n",
       "      <td>0.265037</td>\n",
       "      <td>0.033684</td>\n",
       "      <td>-0.029767</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47096</th>\n",
       "      <td>0.057037</td>\n",
       "      <td>-0.372021</td>\n",
       "      <td>0.189338</td>\n",
       "      <td>0.506027</td>\n",
       "      <td>-0.176856</td>\n",
       "      <td>-0.828299</td>\n",
       "      <td>-0.774709</td>\n",
       "      <td>-0.734041</td>\n",
       "      <td>-0.013954</td>\n",
       "      <td>0.444043</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71956</th>\n",
       "      <td>-0.900382</td>\n",
       "      <td>0.292192</td>\n",
       "      <td>0.742965</td>\n",
       "      <td>-0.558140</td>\n",
       "      <td>-0.265279</td>\n",
       "      <td>0.115898</td>\n",
       "      <td>-0.778816</td>\n",
       "      <td>0.657659</td>\n",
       "      <td>0.206004</td>\n",
       "      <td>-0.447815</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84795</th>\n",
       "      <td>0.338687</td>\n",
       "      <td>0.148958</td>\n",
       "      <td>0.157310</td>\n",
       "      <td>0.775295</td>\n",
       "      <td>0.333494</td>\n",
       "      <td>-0.792170</td>\n",
       "      <td>-0.576099</td>\n",
       "      <td>0.095555</td>\n",
       "      <td>-0.685807</td>\n",
       "      <td>0.941458</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23493</th>\n",
       "      <td>0.874637</td>\n",
       "      <td>0.398489</td>\n",
       "      <td>-0.386605</td>\n",
       "      <td>0.719660</td>\n",
       "      <td>0.668492</td>\n",
       "      <td>-0.009923</td>\n",
       "      <td>-0.322420</td>\n",
       "      <td>-0.191939</td>\n",
       "      <td>0.761743</td>\n",
       "      <td>0.201788</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83490</th>\n",
       "      <td>-0.175773</td>\n",
       "      <td>0.912172</td>\n",
       "      <td>-0.563137</td>\n",
       "      <td>0.529161</td>\n",
       "      <td>0.407686</td>\n",
       "      <td>-0.710036</td>\n",
       "      <td>-0.967755</td>\n",
       "      <td>0.955782</td>\n",
       "      <td>0.083621</td>\n",
       "      <td>-0.051688</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79596</th>\n",
       "      <td>0.416477</td>\n",
       "      <td>-0.826628</td>\n",
       "      <td>0.091526</td>\n",
       "      <td>-0.169211</td>\n",
       "      <td>0.096359</td>\n",
       "      <td>0.906264</td>\n",
       "      <td>0.499372</td>\n",
       "      <td>0.939270</td>\n",
       "      <td>-0.085170</td>\n",
       "      <td>0.265701</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34717</th>\n",
       "      <td>0.511446</td>\n",
       "      <td>0.150669</td>\n",
       "      <td>0.909701</td>\n",
       "      <td>-0.652426</td>\n",
       "      <td>0.341461</td>\n",
       "      <td>0.712089</td>\n",
       "      <td>0.119273</td>\n",
       "      <td>0.621494</td>\n",
       "      <td>-0.079138</td>\n",
       "      <td>0.699302</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37761</th>\n",
       "      <td>0.014751</td>\n",
       "      <td>0.138158</td>\n",
       "      <td>-0.481992</td>\n",
       "      <td>-0.402158</td>\n",
       "      <td>-0.879800</td>\n",
       "      <td>-0.236924</td>\n",
       "      <td>-0.510531</td>\n",
       "      <td>-0.165816</td>\n",
       "      <td>-0.900312</td>\n",
       "      <td>-0.923362</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27730</th>\n",
       "      <td>-0.693767</td>\n",
       "      <td>-0.720884</td>\n",
       "      <td>-0.878092</td>\n",
       "      <td>-0.424711</td>\n",
       "      <td>0.010973</td>\n",
       "      <td>0.873181</td>\n",
       "      <td>0.133849</td>\n",
       "      <td>0.617350</td>\n",
       "      <td>-0.584812</td>\n",
       "      <td>-0.980956</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             X0        X1        X2        X3        X4        X5        X6  \\\n",
       "81168 -0.531767 -0.531129 -0.318579 -0.524463 -0.704684 -0.711323  0.280344   \n",
       "47096  0.057037 -0.372021  0.189338  0.506027 -0.176856 -0.828299 -0.774709   \n",
       "71956 -0.900382  0.292192  0.742965 -0.558140 -0.265279  0.115898 -0.778816   \n",
       "84795  0.338687  0.148958  0.157310  0.775295  0.333494 -0.792170 -0.576099   \n",
       "23493  0.874637  0.398489 -0.386605  0.719660  0.668492 -0.009923 -0.322420   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "83490 -0.175773  0.912172 -0.563137  0.529161  0.407686 -0.710036 -0.967755   \n",
       "79596  0.416477 -0.826628  0.091526 -0.169211  0.096359  0.906264  0.499372   \n",
       "34717  0.511446  0.150669  0.909701 -0.652426  0.341461  0.712089  0.119273   \n",
       "37761  0.014751  0.138158 -0.481992 -0.402158 -0.879800 -0.236924 -0.510531   \n",
       "27730 -0.693767 -0.720884 -0.878092 -0.424711  0.010973  0.873181  0.133849   \n",
       "\n",
       "             X7        X8        X9  z0  \n",
       "81168  0.265037  0.033684 -0.029767  83  \n",
       "47096 -0.734041 -0.013954  0.444043  49  \n",
       "71956  0.657659  0.206004 -0.447815  73  \n",
       "84795  0.095555 -0.685807  0.941458  87  \n",
       "23493 -0.191939  0.761743  0.201788  23  \n",
       "...         ...       ...       ...  ..  \n",
       "83490  0.955782  0.083621 -0.051688  85  \n",
       "79596  0.939270 -0.085170  0.265701  81  \n",
       "34717  0.621494 -0.079138  0.699302  37  \n",
       "37761 -0.165816 -0.900312 -0.923362  41  \n",
       "27730  0.617350 -0.584812 -0.980956  28  \n",
       "\n",
       "[100000 rows x 11 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([X_train, X_test], ignore_index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81168    1\n",
       "47096    0\n",
       "71956    0\n",
       "84795    1\n",
       "23493    0\n",
       "        ..\n",
       "83490    1\n",
       "79596    0\n",
       "34717    0\n",
       "37761    1\n",
       "27730    1\n",
       "Name: y, Length: 100000, dtype: int32"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = pd.concat([y_train, y_test], ignore_index=False)\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>z0</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81168</th>\n",
       "      <td>-0.531767</td>\n",
       "      <td>-0.531129</td>\n",
       "      <td>-0.318579</td>\n",
       "      <td>-0.524463</td>\n",
       "      <td>-0.704684</td>\n",
       "      <td>-0.711323</td>\n",
       "      <td>0.280344</td>\n",
       "      <td>0.265037</td>\n",
       "      <td>0.033684</td>\n",
       "      <td>-0.029767</td>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47096</th>\n",
       "      <td>0.057037</td>\n",
       "      <td>-0.372021</td>\n",
       "      <td>0.189338</td>\n",
       "      <td>0.506027</td>\n",
       "      <td>-0.176856</td>\n",
       "      <td>-0.828299</td>\n",
       "      <td>-0.774709</td>\n",
       "      <td>-0.734041</td>\n",
       "      <td>-0.013954</td>\n",
       "      <td>0.444043</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71956</th>\n",
       "      <td>-0.900382</td>\n",
       "      <td>0.292192</td>\n",
       "      <td>0.742965</td>\n",
       "      <td>-0.558140</td>\n",
       "      <td>-0.265279</td>\n",
       "      <td>0.115898</td>\n",
       "      <td>-0.778816</td>\n",
       "      <td>0.657659</td>\n",
       "      <td>0.206004</td>\n",
       "      <td>-0.447815</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84795</th>\n",
       "      <td>0.338687</td>\n",
       "      <td>0.148958</td>\n",
       "      <td>0.157310</td>\n",
       "      <td>0.775295</td>\n",
       "      <td>0.333494</td>\n",
       "      <td>-0.792170</td>\n",
       "      <td>-0.576099</td>\n",
       "      <td>0.095555</td>\n",
       "      <td>-0.685807</td>\n",
       "      <td>0.941458</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23493</th>\n",
       "      <td>0.874637</td>\n",
       "      <td>0.398489</td>\n",
       "      <td>-0.386605</td>\n",
       "      <td>0.719660</td>\n",
       "      <td>0.668492</td>\n",
       "      <td>-0.009923</td>\n",
       "      <td>-0.322420</td>\n",
       "      <td>-0.191939</td>\n",
       "      <td>0.761743</td>\n",
       "      <td>0.201788</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83490</th>\n",
       "      <td>-0.175773</td>\n",
       "      <td>0.912172</td>\n",
       "      <td>-0.563137</td>\n",
       "      <td>0.529161</td>\n",
       "      <td>0.407686</td>\n",
       "      <td>-0.710036</td>\n",
       "      <td>-0.967755</td>\n",
       "      <td>0.955782</td>\n",
       "      <td>0.083621</td>\n",
       "      <td>-0.051688</td>\n",
       "      <td>85</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79596</th>\n",
       "      <td>0.416477</td>\n",
       "      <td>-0.826628</td>\n",
       "      <td>0.091526</td>\n",
       "      <td>-0.169211</td>\n",
       "      <td>0.096359</td>\n",
       "      <td>0.906264</td>\n",
       "      <td>0.499372</td>\n",
       "      <td>0.939270</td>\n",
       "      <td>-0.085170</td>\n",
       "      <td>0.265701</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34717</th>\n",
       "      <td>0.511446</td>\n",
       "      <td>0.150669</td>\n",
       "      <td>0.909701</td>\n",
       "      <td>-0.652426</td>\n",
       "      <td>0.341461</td>\n",
       "      <td>0.712089</td>\n",
       "      <td>0.119273</td>\n",
       "      <td>0.621494</td>\n",
       "      <td>-0.079138</td>\n",
       "      <td>0.699302</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37761</th>\n",
       "      <td>0.014751</td>\n",
       "      <td>0.138158</td>\n",
       "      <td>-0.481992</td>\n",
       "      <td>-0.402158</td>\n",
       "      <td>-0.879800</td>\n",
       "      <td>-0.236924</td>\n",
       "      <td>-0.510531</td>\n",
       "      <td>-0.165816</td>\n",
       "      <td>-0.900312</td>\n",
       "      <td>-0.923362</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27730</th>\n",
       "      <td>-0.693767</td>\n",
       "      <td>-0.720884</td>\n",
       "      <td>-0.878092</td>\n",
       "      <td>-0.424711</td>\n",
       "      <td>0.010973</td>\n",
       "      <td>0.873181</td>\n",
       "      <td>0.133849</td>\n",
       "      <td>0.617350</td>\n",
       "      <td>-0.584812</td>\n",
       "      <td>-0.980956</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             X0        X1        X2        X3        X4        X5        X6  \\\n",
       "81168 -0.531767 -0.531129 -0.318579 -0.524463 -0.704684 -0.711323  0.280344   \n",
       "47096  0.057037 -0.372021  0.189338  0.506027 -0.176856 -0.828299 -0.774709   \n",
       "71956 -0.900382  0.292192  0.742965 -0.558140 -0.265279  0.115898 -0.778816   \n",
       "84795  0.338687  0.148958  0.157310  0.775295  0.333494 -0.792170 -0.576099   \n",
       "23493  0.874637  0.398489 -0.386605  0.719660  0.668492 -0.009923 -0.322420   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "83490 -0.175773  0.912172 -0.563137  0.529161  0.407686 -0.710036 -0.967755   \n",
       "79596  0.416477 -0.826628  0.091526 -0.169211  0.096359  0.906264  0.499372   \n",
       "34717  0.511446  0.150669  0.909701 -0.652426  0.341461  0.712089  0.119273   \n",
       "37761  0.014751  0.138158 -0.481992 -0.402158 -0.879800 -0.236924 -0.510531   \n",
       "27730 -0.693767 -0.720884 -0.878092 -0.424711  0.010973  0.873181  0.133849   \n",
       "\n",
       "             X7        X8        X9  z0  label  \n",
       "81168  0.265037  0.033684 -0.029767  83      1  \n",
       "47096 -0.734041 -0.013954  0.444043  49      0  \n",
       "71956  0.657659  0.206004 -0.447815  73      0  \n",
       "84795  0.095555 -0.685807  0.941458  87      1  \n",
       "23493 -0.191939  0.761743  0.201788  23      0  \n",
       "...         ...       ...       ...  ..    ...  \n",
       "83490  0.955782  0.083621 -0.051688  85      1  \n",
       "79596  0.939270 -0.085170  0.265701  81      0  \n",
       "34717  0.621494 -0.079138  0.699302  37      0  \n",
       "37761 -0.165816 -0.900312 -0.923362  41      1  \n",
       "27730  0.617350 -0.584812 -0.980956  28      1  \n",
       "\n",
       "[100000 rows x 12 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'] = target\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('Synthetic_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>z0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81168</th>\n",
       "      <td>-0.531767</td>\n",
       "      <td>-0.531129</td>\n",
       "      <td>-0.318579</td>\n",
       "      <td>-0.524463</td>\n",
       "      <td>-0.704684</td>\n",
       "      <td>-0.711323</td>\n",
       "      <td>0.280344</td>\n",
       "      <td>0.265037</td>\n",
       "      <td>0.033684</td>\n",
       "      <td>-0.029767</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47096</th>\n",
       "      <td>0.057037</td>\n",
       "      <td>-0.372021</td>\n",
       "      <td>0.189338</td>\n",
       "      <td>0.506027</td>\n",
       "      <td>-0.176856</td>\n",
       "      <td>-0.828299</td>\n",
       "      <td>-0.774709</td>\n",
       "      <td>-0.734041</td>\n",
       "      <td>-0.013954</td>\n",
       "      <td>0.444043</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71956</th>\n",
       "      <td>-0.900382</td>\n",
       "      <td>0.292192</td>\n",
       "      <td>0.742965</td>\n",
       "      <td>-0.558140</td>\n",
       "      <td>-0.265279</td>\n",
       "      <td>0.115898</td>\n",
       "      <td>-0.778816</td>\n",
       "      <td>0.657659</td>\n",
       "      <td>0.206004</td>\n",
       "      <td>-0.447815</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84795</th>\n",
       "      <td>0.338687</td>\n",
       "      <td>0.148958</td>\n",
       "      <td>0.157310</td>\n",
       "      <td>0.775295</td>\n",
       "      <td>0.333494</td>\n",
       "      <td>-0.792170</td>\n",
       "      <td>-0.576099</td>\n",
       "      <td>0.095555</td>\n",
       "      <td>-0.685807</td>\n",
       "      <td>0.941458</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23493</th>\n",
       "      <td>0.874637</td>\n",
       "      <td>0.398489</td>\n",
       "      <td>-0.386605</td>\n",
       "      <td>0.719660</td>\n",
       "      <td>0.668492</td>\n",
       "      <td>-0.009923</td>\n",
       "      <td>-0.322420</td>\n",
       "      <td>-0.191939</td>\n",
       "      <td>0.761743</td>\n",
       "      <td>0.201788</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78193</th>\n",
       "      <td>-0.339709</td>\n",
       "      <td>-0.933041</td>\n",
       "      <td>0.415370</td>\n",
       "      <td>-0.166107</td>\n",
       "      <td>-0.574307</td>\n",
       "      <td>0.250758</td>\n",
       "      <td>-0.354339</td>\n",
       "      <td>-0.781210</td>\n",
       "      <td>-0.542049</td>\n",
       "      <td>-0.358265</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40052</th>\n",
       "      <td>-0.691073</td>\n",
       "      <td>-0.592864</td>\n",
       "      <td>-0.403589</td>\n",
       "      <td>0.171708</td>\n",
       "      <td>0.594380</td>\n",
       "      <td>-0.506743</td>\n",
       "      <td>-0.347860</td>\n",
       "      <td>0.772321</td>\n",
       "      <td>0.661877</td>\n",
       "      <td>-0.433515</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33410</th>\n",
       "      <td>-0.313796</td>\n",
       "      <td>0.401396</td>\n",
       "      <td>-0.469474</td>\n",
       "      <td>-0.724392</td>\n",
       "      <td>0.018785</td>\n",
       "      <td>-0.665830</td>\n",
       "      <td>0.930409</td>\n",
       "      <td>0.229737</td>\n",
       "      <td>0.836275</td>\n",
       "      <td>0.030957</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10498</th>\n",
       "      <td>0.161213</td>\n",
       "      <td>-0.039404</td>\n",
       "      <td>0.314033</td>\n",
       "      <td>-0.447159</td>\n",
       "      <td>0.578798</td>\n",
       "      <td>-0.092059</td>\n",
       "      <td>-0.468478</td>\n",
       "      <td>0.502907</td>\n",
       "      <td>-0.733544</td>\n",
       "      <td>-0.527874</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26350</th>\n",
       "      <td>-0.409752</td>\n",
       "      <td>0.794351</td>\n",
       "      <td>-0.357318</td>\n",
       "      <td>-0.720638</td>\n",
       "      <td>0.964123</td>\n",
       "      <td>-0.418051</td>\n",
       "      <td>-0.477461</td>\n",
       "      <td>0.640088</td>\n",
       "      <td>-0.794615</td>\n",
       "      <td>-0.143932</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             X0        X1        X2        X3        X4        X5        X6  \\\n",
       "81168 -0.531767 -0.531129 -0.318579 -0.524463 -0.704684 -0.711323  0.280344   \n",
       "47096  0.057037 -0.372021  0.189338  0.506027 -0.176856 -0.828299 -0.774709   \n",
       "71956 -0.900382  0.292192  0.742965 -0.558140 -0.265279  0.115898 -0.778816   \n",
       "84795  0.338687  0.148958  0.157310  0.775295  0.333494 -0.792170 -0.576099   \n",
       "23493  0.874637  0.398489 -0.386605  0.719660  0.668492 -0.009923 -0.322420   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "78193 -0.339709 -0.933041  0.415370 -0.166107 -0.574307  0.250758 -0.354339   \n",
       "40052 -0.691073 -0.592864 -0.403589  0.171708  0.594380 -0.506743 -0.347860   \n",
       "33410 -0.313796  0.401396 -0.469474 -0.724392  0.018785 -0.665830  0.930409   \n",
       "10498  0.161213 -0.039404  0.314033 -0.447159  0.578798 -0.092059 -0.468478   \n",
       "26350 -0.409752  0.794351 -0.357318 -0.720638  0.964123 -0.418051 -0.477461   \n",
       "\n",
       "             X7        X8        X9  z0  \n",
       "81168  0.265037  0.033684 -0.029767  83  \n",
       "47096 -0.734041 -0.013954  0.444043  49  \n",
       "71956  0.657659  0.206004 -0.447815  73  \n",
       "84795  0.095555 -0.685807  0.941458  87  \n",
       "23493 -0.191939  0.761743  0.201788  23  \n",
       "...         ...       ...       ...  ..  \n",
       "78193 -0.781210 -0.542049 -0.358265  80  \n",
       "40052  0.772321  0.661877 -0.433515  43  \n",
       "33410  0.229737  0.836275  0.030957  35  \n",
       "10498  0.502907 -0.733544 -0.527874  10  \n",
       "26350  0.640088 -0.794615 -0.143932  26  \n",
       "\n",
       "[80000 rows x 11 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81168    1\n",
       "47096    0\n",
       "71956    0\n",
       "84795    1\n",
       "23493    0\n",
       "        ..\n",
       "78193    0\n",
       "40052    1\n",
       "33410    1\n",
       "10498    0\n",
       "26350    1\n",
       "Name: y, Length: 80000, dtype: int32"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['X0', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
       "       68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n",
       "       85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(X_train['z0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(y_train))\n",
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_48 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.8103 - accuracy: 0.5729\n",
      "Epoch 2/20\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.6552 - accuracy: 0.5973\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.6469 - accuracy: 0.5984\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.6416 - accuracy: 0.6097\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.6374 - accuracy: 0.6198\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.6340 - accuracy: 0.6283\n",
      "Epoch 7/20\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.6296 - accuracy: 0.6362\n",
      "Epoch 8/20\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.6266 - accuracy: 0.6421\n",
      "Epoch 9/20\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.6247 - accuracy: 0.6457\n",
      "Epoch 10/20\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.6239 - accuracy: 0.6483\n",
      "Epoch 11/20\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.6225 - accuracy: 0.6500\n",
      "Epoch 12/20\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.6220 - accuracy: 0.6521\n",
      "Epoch 13/20\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.6215 - accuracy: 0.6534\n",
      "Epoch 14/20\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.6212 - accuracy: 0.6531\n",
      "Epoch 15/20\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.6210 - accuracy: 0.6533\n",
      "Epoch 16/20\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.6202 - accuracy: 0.6549\n",
      "Epoch 17/20\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.6207 - accuracy: 0.6541\n",
      "Epoch 18/20\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.6199 - accuracy: 0.6559\n",
      "Epoch 19/20\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.6198 - accuracy: 0.6551\n",
      "Epoch 20/20\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.6200 - accuracy: 0.6555\n",
      "Accuracy: 66.00%\n"
     ]
    }
   ],
   "source": [
    "# simple model using only one z variable\n",
    "model_simple = Sequential()\n",
    "model_simple.add(Dense(5, activation='relu'))\n",
    "model_simple.add(Dense(2, activation='sigmoid'))\n",
    "model_simple.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "model_simple.fit(X_train, y_train, epochs=20, batch_size=256)\n",
    "# Final evaluation of the model\n",
    "scores = model_simple.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_50 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.6880 - accuracy: 0.5245\n",
      "Epoch 2/20\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.6588 - accuracy: 0.5777\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 0.6455 - accuracy: 0.5807\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.6387 - accuracy: 0.5860\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.6330 - accuracy: 0.5922\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.6261 - accuracy: 0.5964\n",
      "Epoch 7/20\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.6172 - accuracy: 0.5995\n",
      "Epoch 8/20\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.6124 - accuracy: 0.6019\n",
      "Epoch 9/20\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.6093 - accuracy: 0.6025\n",
      "Epoch 10/20\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.6076 - accuracy: 0.6118\n",
      "Epoch 11/20\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.6042 - accuracy: 0.6307\n",
      "Epoch 12/20\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.6019 - accuracy: 0.6351\n",
      "Epoch 13/20\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.6004 - accuracy: 0.6366\n",
      "Epoch 14/20\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.5976 - accuracy: 0.6391\n",
      "Epoch 15/20\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.5965 - accuracy: 0.6403\n",
      "Epoch 16/20\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.5950 - accuracy: 0.6416\n",
      "Epoch 17/20\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5930 - accuracy: 0.6438\n",
      "Epoch 18/20\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5925 - accuracy: 0.6439\n",
      "Epoch 19/20\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.5918 - accuracy: 0.6441\n",
      "Epoch 20/20\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.5911 - accuracy: 0.6452\n",
      "Accuracy: 64.68%\n"
     ]
    }
   ],
   "source": [
    "# more complex model using 1 z variable\n",
    "model_simple = Sequential()\n",
    "model_simple.add(Dense(10, activation='relu'))\n",
    "model_simple.add(Dense(7, activation='relu'))\n",
    "model_simple.add(Dense(4, activation='relu'))\n",
    "model_simple.add(Dense(2, activation='sigmoid'))\n",
    "model_simple.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "model_simple.fit(X_train, y_train, epochs=20, batch_size=256)\n",
    "# Final evaluation of the model\n",
    "scores = model_simple.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2000/2000 [==============================] - 5s 3ms/step - loss: 1.3688 - accuracy: 0.5435\n",
      "Epoch 2/20\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.6883 - accuracy: 0.5493\n",
      "Epoch 3/20\n",
      "2000/2000 [==============================] - 5s 3ms/step - loss: 0.6883 - accuracy: 0.5493\n",
      "Epoch 4/20\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.6883 - accuracy: 0.5493\n",
      "Epoch 5/20\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.6883 - accuracy: 0.5493\n",
      "Epoch 6/20\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.6899 - accuracy: 0.5493\n",
      "Epoch 7/20\n",
      "2000/2000 [==============================] - 5s 3ms/step - loss: 0.6883 - accuracy: 0.5493\n",
      "Epoch 8/20\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.6883 - accuracy: 0.5493\n",
      "Epoch 9/20\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.6883 - accuracy: 0.5493\n",
      "Epoch 10/20\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.6883 - accuracy: 0.5493\n",
      "Epoch 11/20\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.6883 - accuracy: 0.5493\n",
      "Epoch 12/20\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.6883 - accuracy: 0.5493\n",
      "Epoch 13/20\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.6883 - accuracy: 0.5493\n",
      "Epoch 14/20\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.6883 - accuracy: 0.5493\n",
      "Epoch 15/20\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.6883 - accuracy: 0.5493\n",
      "Epoch 16/20\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.6882 - accuracy: 0.5493\n",
      "Epoch 17/20\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.6882 - accuracy: 0.5493\n",
      "Epoch 18/20\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.6882 - accuracy: 0.5493\n",
      "Epoch 19/20\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.6882 - accuracy: 0.5493\n",
      "Epoch 20/20\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.6882 - accuracy: 0.5493\n",
      "Accuracy: 54.78%\n"
     ]
    }
   ],
   "source": [
    "# simple model using 3 z variables\n",
    "model_simple = Sequential()\n",
    "model_simple.add(Dense(5, activation='relu'))\n",
    "model_simple.add(Dense(2, activation='sigmoid'))\n",
    "model_simple.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "model_simple.fit(X_train, y_train, epochs=20, batch_size=40)\n",
    "# Final evaluation of the model\n",
    "scores = model_simple.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_8 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2500/2500 [==============================] - 9s 4ms/step - loss: 0.6931 - accuracy: 0.5492\n",
      "Epoch 2/20\n",
      "2500/2500 [==============================] - 8s 3ms/step - loss: 0.6931 - accuracy: 0.5493\n",
      "Epoch 3/20\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.6889 - accuracy: 0.5494\n",
      "Epoch 4/20\n",
      "2500/2500 [==============================] - ETA: 0s - loss: 0.7013 - accuracy: 0.54 - 7s 3ms/step - loss: 0.7012 - accuracy: 0.5447\n",
      "Epoch 5/20\n",
      "2500/2500 [==============================] - 8s 3ms/step - loss: 0.6933 - accuracy: 0.5416\n",
      "Epoch 6/20\n",
      "2500/2500 [==============================] - 8s 3ms/step - loss: 0.6932 - accuracy: 0.5438\n",
      "Epoch 7/20\n",
      "2500/2500 [==============================] - 8s 3ms/step - loss: 0.6931 - accuracy: 0.5464\n",
      "Epoch 8/20\n",
      "2500/2500 [==============================] - 8s 3ms/step - loss: 0.6930 - accuracy: 0.5461\n",
      "Epoch 9/20\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.6933 - accuracy: 0.5469\n",
      "Epoch 10/20\n",
      "2500/2500 [==============================] - 9s 4ms/step - loss: 0.6932 - accuracy: 0.5436\n",
      "Epoch 11/20\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 0.6930 - accuracy: 0.5451\n",
      "Epoch 12/20\n",
      "2500/2500 [==============================] - 9s 4ms/step - loss: 0.6930 - accuracy: 0.5450\n",
      "Epoch 13/20\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 0.6930 - accuracy: 0.5453\n",
      "Epoch 14/20\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 0.6930 - accuracy: 0.5457\n",
      "Epoch 15/20\n",
      "2500/2500 [==============================] - 8s 3ms/step - loss: 0.6929 - accuracy: 0.5406\n",
      "Epoch 16/20\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.6929 - accuracy: 0.5455\n",
      "Epoch 17/20\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.6928 - accuracy: 0.5470\n",
      "Epoch 18/20\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.6928 - accuracy: 0.5459\n",
      "Epoch 19/20\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.6928 - accuracy: 0.5486\n",
      "Epoch 20/20\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 0.6928 - accuracy: 0.5457\n",
      "Accuracy: 54.77%\n"
     ]
    }
   ],
   "source": [
    "# more complex model using 3 z variables\n",
    "model_simple = Sequential()\n",
    "model_simple.add(Dense(10, activation='relu'))\n",
    "model_simple.add(Dense(7, activation='relu'))\n",
    "model_simple.add(Dense(4, activation='relu'))\n",
    "model_simple.add(Dense(2, activation='sigmoid'))\n",
    "model_simple.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "model_simple.fit(X_train, y_train, epochs=20, batch_size=256)\n",
    "# Final evaluation of the model\n",
    "scores = model_simple.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6599"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest using 1 z variable\n",
    "\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57825"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest using 3 z variables\n",
    "\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['X0', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'z0'], dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_set = X_train.columns\n",
    "col_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[col_set].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "282/282 [==============================] - 27s 96ms/step - loss: 199.9527 - val_loss: 194.5889\n",
      "Epoch 2/10\n",
      "282/282 [==============================] - 26s 93ms/step - loss: 199.5365 - val_loss: 194.1446\n",
      "Epoch 3/10\n",
      "282/282 [==============================] - 32s 113ms/step - loss: 198.8748 - val_loss: 193.9037\n",
      "Epoch 4/10\n",
      "282/282 [==============================] - 25s 87ms/step - loss: 198.6808 - val_loss: 193.7709\n",
      "Epoch 5/10\n",
      "282/282 [==============================] - 27s 95ms/step - loss: 195.0841 - val_loss: 180.9470\n",
      "Epoch 6/10\n",
      "282/282 [==============================] - 27s 97ms/step - loss: 184.3875 - val_loss: 179.0182\n",
      "Epoch 7/10\n",
      "282/282 [==============================] - 26s 93ms/step - loss: 182.6863 - val_loss: 177.7023\n",
      "Epoch 8/10\n",
      "282/282 [==============================] - 27s 96ms/step - loss: 181.1982 - val_loss: 176.6341\n",
      "Epoch 9/10\n",
      "282/282 [==============================] - 26s 94ms/step - loss: 180.5498 - val_loss: 175.7048\n",
      "Epoch 10/10\n",
      "282/282 [==============================] - 22s 78ms/step - loss: 179.5828 - val_loss: 174.9943\n"
     ]
    }
   ],
   "source": [
    "# LMMNN using 3 z variables\n",
    "\n",
    "mode = 'glmm'\n",
    "NUM_ITERATIONS = 1000\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.001\n",
    "DECAY_RATE = 0.0001\n",
    "\n",
    "X_input = Input(shape=(X_train[col_set].shape[1],))\n",
    "y_true_input = Input(shape=(1,))\n",
    "Z_input = Input(shape=(1,), dtype=tf.int64)\n",
    "hidden1 = Dense(units=10, activation='relu', input_dim=len(X_train[col_set].values[0]))(X_input)\n",
    "hidden2 = Dense(units=5, activation='relu')(hidden1)\n",
    "y_pred_output = Dense(units=1, activation='sigmoid')(hidden2)\n",
    "nll = NLL(mode, 1.0, [1.0])(y_true_input, y_pred_output, [Z_input])\n",
    "model = Model(inputs=[X_input, y_true_input, Z_input], outputs=nll)\n",
    "\n",
    "adam = Adam(learning_rate=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=None, decay=DECAY_RATE, amsgrad=False)\n",
    "model.compile(optimizer = adam)\n",
    "# X_train.reset_index(inplace=True)\n",
    "# X_train.sort_values(by=['z0'], inplace=True)\n",
    "# y_train = y_train[X_train.index]\n",
    "\n",
    "history = model.fit([X_train[col_set], y_train, X_train['z0']], None, batch_size=BATCH_SIZE, epochs=10,\n",
    "                    validation_split=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 8s 14ms/step - loss: 22.1788\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22.178842544555664"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([X_test[col_set], y_test, X_test['z0']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "282/282 [==============================] - 36s 129ms/step - loss: 180.8340 - val_loss: 175.4104\n",
      "Epoch 2/10\n",
      "282/282 [==============================] - 36s 127ms/step - loss: 178.2587 - val_loss: 174.1581\n",
      "Epoch 3/10\n",
      "282/282 [==============================] - 28s 100ms/step - loss: 177.1214 - val_loss: 173.4776\n",
      "Epoch 4/10\n",
      "282/282 [==============================] - 40s 141ms/step - loss: 176.8749 - val_loss: 173.1710\n",
      "Epoch 5/10\n",
      "282/282 [==============================] - 40s 142ms/step - loss: 176.6951 - val_loss: 173.0549\n",
      "Epoch 6/10\n",
      "282/282 [==============================] - 37s 132ms/step - loss: 175.8944 - val_loss: 170.5000\n",
      "Epoch 7/10\n",
      "282/282 [==============================] - 34s 122ms/step - loss: 173.5064 - val_loss: 169.7025\n",
      "Epoch 8/10\n",
      "282/282 [==============================] - 37s 132ms/step - loss: 173.2585 - val_loss: 169.5752\n",
      "Epoch 9/10\n",
      "282/282 [==============================] - 33s 119ms/step - loss: 173.2996 - val_loss: 169.5060\n",
      "Epoch 10/10\n",
      "282/282 [==============================] - 32s 113ms/step - loss: 173.1255 - val_loss: 169.4171\n"
     ]
    }
   ],
   "source": [
    "# LMMNN using 1 z variable\n",
    "\n",
    "mode = 'glmm'\n",
    "NUM_ITERATIONS = 1000\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.001\n",
    "DECAY_RATE = 0.0001\n",
    "\n",
    "X_input = Input(shape=(X_train[col_set].shape[1],))\n",
    "y_true_input = Input(shape=(1,))\n",
    "Z_input = Input(shape=(1,), dtype=tf.int64)\n",
    "hidden1 = Dense(units=10, activation='relu', input_dim=len(X_train[col_set].values[0]))(X_input)\n",
    "hidden2 = Dense(units=5, activation='relu')(hidden1)\n",
    "y_pred_output = Dense(units=1, activation='sigmoid')(hidden2)\n",
    "nll = NLL(mode, 1.0, [1.0])(y_true_input, y_pred_output, [Z_input])\n",
    "model = Model(inputs=[X_input, y_true_input, Z_input], outputs=nll)\n",
    "\n",
    "adam = Adam(learning_rate=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=None, decay=DECAY_RATE, amsgrad=False)\n",
    "model.compile(optimizer = adam)\n",
    "\n",
    "# X_train.reset_index(inplace=True)\n",
    "# X_train.sort_values(by=['z0'], inplace=True)\n",
    "# y_train = y_train[X_train.index]\n",
    "\n",
    "history = model.fit([X_train[col_set], y_train, X_train['z0']], None, batch_size=BATCH_SIZE, epochs=10,\n",
    "                    validation_split=0.1, shuffle=True)\n",
    "\n",
    "# model.fit([X_train[col_set], y_train, X_train['z0']], None, epochs=20, batch_size=BATCH_SIZE)\n",
    "# # Final evaluation of the model\n",
    "# scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "# print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 3s 4ms/step - loss: 21.7560\n",
      "21.756011962890625\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate([X_test[col_set], y_test, X_test['z0']])\n",
    "print(scores)\n",
    "#print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Model using function reg_nn_lmm from nn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = [col for col in X_train.columns if col not in ['z0','z1','z2']]\n",
    "batch = 100\n",
    "epochs = 10\n",
    "patience = 10\n",
    "qs=[100]#,1000,10000]\n",
    "q_spatial=[]\n",
    "n_neurons = [10, 5, 2]\n",
    "dropout = None\n",
    "activation = 'relu'\n",
    "mode = 'glmm'\n",
    "n_sig2bs = 1\n",
    "n_sig2bs_spatial = 0\n",
    "est_cors = []\n",
    "dist_matrix = None\n",
    "time2measure_dict = None\n",
    "spatial_embed_neurons = None\n",
    "verbose = True\n",
    "Z_non_linear = False\n",
    "log_params = False\n",
    "idx = None\n",
    "Z_embed_dim_pct = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 67.0956 - val_loss: 65.0391\n",
      "Epoch 2/10\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 64.4057 - val_loss: 63.5360\n",
      "Epoch 3/10\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 63.0351 - val_loss: 62.5357\n",
      "Epoch 4/10\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 62.3010 - val_loss: 62.1152\n",
      "Epoch 5/10\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 61.9573 - val_loss: 61.9300\n",
      "Epoch 6/10\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 61.7883 - val_loss: 61.8486\n",
      "Epoch 7/10\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 61.7039 - val_loss: 61.8190\n",
      "Epoch 8/10\n",
      "720/720 [==============================] - 5s 7ms/step - loss: 61.6554 - val_loss: 61.7972\n",
      "Epoch 9/10\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 61.6223 - val_loss: 61.7755\n",
      "Epoch 10/10\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 61.5933 - val_loss: 61.7532\n"
     ]
    }
   ],
   "source": [
    "# simple\n",
    "n_neurons = [5]\n",
    "y_pred, sigmas, rhos, weibull, n_epochs = reg_nn_lmm(\n",
    "            X_train, X_test, y_train, y_test, qs, q_spatial, x_cols, batch, epochs, patience,\n",
    "            n_neurons, dropout, activation, mode,\n",
    "            n_sig2bs, n_sig2bs_spatial, est_cors, dist_matrix, spatial_embed_neurons, verbose, Z_non_linear, Z_embed_dim_pct, log_params, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65925"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# round predictions to closest integer\n",
    "y_pred_class = []\n",
    "for n in y_pred:\n",
    "    y_pred_class.append(round(n))\n",
    "\n",
    "accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 67.9445 - val_loss: 66.3830\n",
      "Epoch 2/10\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 63.2507 - val_loss: 62.3688\n",
      "Epoch 3/10\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 61.0069 - val_loss: 60.9193\n",
      "Epoch 4/10\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 60.0911 - val_loss: 60.2229\n",
      "Epoch 5/10\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 59.4728 - val_loss: 59.5839\n",
      "Epoch 6/10\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 58.8078 - val_loss: 58.7403\n",
      "Epoch 7/10\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 58.1545 - val_loss: 58.2428\n",
      "Epoch 8/10\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 57.8414 - val_loss: 58.0293\n",
      "Epoch 9/10\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 57.6636 - val_loss: 57.8732\n",
      "Epoch 10/10\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 57.5537 - val_loss: 57.7671\n"
     ]
    }
   ],
   "source": [
    "# more complex\n",
    "y_pred, sigmas, rhos, weibull, n_epochs = reg_nn_lmm(\n",
    "            X_train, X_test, y_train, y_test, qs, q_spatial, x_cols, batch, epochs, patience,\n",
    "            n_neurons, dropout, activation, mode,\n",
    "            n_sig2bs, n_sig2bs_spatial, est_cors, dist_matrix, spatial_embed_neurons, verbose, Z_non_linear, Z_embed_dim_pct, log_params, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.30334791, 0.32082577, 0.2263784 , ..., 0.13376543, 0.52185211,\n",
       "       0.153196  ])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91505    1\n",
       "42340    0\n",
       "19847    0\n",
       "22940    1\n",
       "39723    0\n",
       "        ..\n",
       "15236    1\n",
       "22404    0\n",
       "13079    1\n",
       "31917    0\n",
       "6239     0\n",
       "Name: y, Length: 20000, dtype: int32"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred) == len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# round predictions to closest integer\n",
    "y_pred_class = []\n",
    "for n in y_pred:\n",
    "    y_pred_class.append(round(n))\n",
    "\n",
    "y_pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70115"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred_class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "b6b431bae85c5149b096c4cf5f10cadc6dce712bdc15e22278af96c84aa7d121"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
